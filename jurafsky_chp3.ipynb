{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech and Language Processing. Daniel Jurafsky & James H. Martin\n",
    "\n",
    "## Chapter 3 - N-gram Language Models\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "Models that assign probabilities to sequences of words are called language models or LMs.\n",
    "\n",
    "The simples language model is called N-gram. --> Derivations bigram or trigram as specific 2 and 3 words N-gram\n",
    "\n",
    "A bit of probability: we are facing the problem of estimating $P(w|h)$ where, for example, `w` is `the` and `h` is `its water is so transparent`. Supposing we have a corpus, we need to compute:\n",
    "\n",
    "$ P(w|h) = \\frac{P(w \\cap h)}{P(h)} $\n",
    "\n",
    "Guess what, which is the probability of $P(h)$? We need to compute out of all the 5 words phrases (words in sequence, no meaning matters at all) which ones match the given phrase. That's hard...\n",
    "\n",
    "So, another approach, still a heavy one, would be to use a probability chain rule to compute the probability of finding such sequence. Introducing some notation, let $X_i$ be the event of a word $w$ taking a certain value. And let $w^k_i$ be a sequence of words $w_i, w_{i+1}, ... , w_{k}$ in order. The probability of such sequence of words:\n",
    "\n",
    "$P(w^k_1) = \\displaystyle \\prod_{k=1}^{n} P(w_k|w^{k-1}_i)$\n",
    "\n",
    "Still, this is hard to compute.\n",
    "\n",
    "The intuition of the n-gram model is that instead of computing the probability of a word given its entire history, we can approximate the history by just the last few words.\n",
    "\n",
    "**Bigram**\n",
    "\n",
    "The bigram model, for example, approximates the probability of a word given all the previous words $P(w_n | w^{n−1}_1)$ by using only the conditional probability of the preceding word $P(w_n | w_{n−1})$. In other words, instead of computing the probability $P(the | Walden Pond’s water is so transparent that)$ we approximate it with the probability $P(that | the)$, i.e. $P(w_n | w^{n−1}_1) \\approx P(w_n | w_{n−1})$\n",
    "\n",
    "**N-gram**\n",
    "\n",
    "The general equation for this n-gram approximation to the conditional probability of the next word in a sequence is:\n",
    "\n",
    "$P(w_n | w^{n−1}_1) \\approx P(w_n | w_{n−N+1})$\n",
    "\n",
    "And for a complete sequence \n",
    "\n",
    "$P(w^n_1) \\approx \\displaystyle \\prod_{k=1}^{n} P(w_k|w_{k-1})$\n",
    "\n",
    "**Markov assumption**\n",
    "\n",
    "The assumption that the probability of a word depends only on the previous word is called a Markov assumption. Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past.\n",
    "\n",
    "**Maximum Likehood Estimation**\n",
    "\n",
    "We get the MLE estimate for the parameters of an n-gram model by getting counts from a corpus, and normalizing the counts so that they lie between 0 and 1.\n",
    "\n",
    "Let's consider bigrams:\n",
    "\n",
    "$ P(w | w_{n-1}) = \\frac{C(w_{n-1}w_n)}{\\sum_{w}C(w_{n-1}w)} = \\frac{C(w_{n-1}w_n)}{C(w_{n-1})}$\n",
    "\n",
    "And simplify the calculus because each bigram in the denominator within the summatory will add up to the count of $ C(w_{n-1})$.\n",
    "\n",
    "For the general case:\n",
    "\n",
    "$ P(w | w^{n-1}_{n-N+1}) = \\frac{C(w^{n-1}_{n-N+1}w_n)}{C(w^{n-1}_{n-N+1})}$\n",
    "\n",
    "**Practical tips**\n",
    "\n",
    "To learn, use bigrams. For a professional work, use trigram or 4-gram and 5-gram when there is sufficient data.\n",
    "\n",
    "**Evaluating language models**\n",
    "\n",
    "*Extrinsic* evaluation equals to *end-to-end* evaluation which involves embedding the model into the final application and comparing it against another model. This is usually expensive and probably not the best way to do it.\n",
    "\n",
    "*Intrinsic* evaluation is motivated by the trend of a metric that varies with modifications in the model and informs the data scientists how the model will perform in an end-to-end application. This way of working involves using a test and validation set and, sometimes, also a hold out set to validate the final model as it was a final application (E2E application).\n",
    "\n",
    "**Perplexity**\n",
    "\n",
    "A metrix introduced to work, instead of probabilities, with the performance of the model.\n",
    "\n",
    "$PP(W) = P(w_1 w_2 ... w_N)^{-\\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P(w_1 w_2 ... w_N)}} = \\sqrt[N]{\\displaystyle \\prod_{i=1}^{N} \\frac{1}{P(w_i | w_1 ... w_{i-1})}}$\n",
    "\n",
    "Which it is simplified for a bigram to:\n",
    "\n",
    "$PP_{bigram}(W) = \\sqrt[N]{\\displaystyle \\prod_{i=1}^{N} \\frac{1}{P(w_i|w_{i-1})}}$\n",
    "\n",
    "The higher the $PP$, the lower the probability. The lower the $PP$, the higher the probability. We are interested in minimizing the $PP$ to maximize the probability.\n",
    "\n",
    "Another way to think about the perplexity is to consider the *the average branching factor of a language* (or a corpus). The branching factor is the number of words that can follow another word.\n",
    "\n",
    "The more information the n-gram gives us about the word sequence, the lower the perplexity.\n",
    "\n",
    "This magnitude is also related to the *entropy*.\n",
    "\n",
    "An (intrinsic) improvement in perplexity does not guarantee an (extrinsic) improvement in the performance of a language processing task like speech recognition or machine translation. Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm. But a model’s\n",
    "improvement in perplexity should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About models**\n",
    "\n",
    "Having N-gram from a corpus to generate text later has some caveats:\n",
    "\n",
    "- Generated text will be much alike the train set.\n",
    "- Conforming $N$ increases, the variety of the generated text diminishes significantly because the branching factor of a certain N-gram decreases with $N$.\n",
    "- Conforming $N$ increases, the global meaning of the generated text increases. Unigrams and bigrams have too much local meaning.\n",
    "- In the train set we might find zero probability N-gram while N-1-gram appears with certain probability. However, when facing the validation set, those N-grams appear. These zero probability problem due to sparsity and lack of information in the train set are a huge problem whem trying to compute $PP$ (note a division by zero).\n",
    "\n",
    "As a general rule of thumb, whenever you want to predict text of certain genre, use that text genre to train your model. Otherwise, the output of the model will be anything but what you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unknown words**\n",
    "\n",
    "There is a world where we know the entire vocabulary and there is a another world where we don't. The former is called closed vocabulary system. The later is called open vocabulary system and comes with some new metrics. The Out Of Vocabulary (OOV) rate is the number of words that do not exist in vocabulary but do exist in the train data over the total number of words in the vocabulary. In general, those unknown words should be replaced by a token such as `<UNK>`. There are two ways to deal with unknown words probabilities:\n",
    "\n",
    "*Case A*\n",
    "\n",
    "Transform the problem into a closed vocabulary system again by:\n",
    "\n",
    "- Fix the vocabulary.\n",
    "- Convert any OOV word into `<UNK>` as a step of text normalization.\n",
    "- Now, `<UNK>` is another word in the vocabulary. Count it as such.\n",
    "\n",
    "*Case B*\n",
    "\n",
    "There is no prior vocabulary set. Consequently, there is a first pass over the corpus that defines such vocabulary and counts token occurrences. All tokens whose count is less that *n* are treated as `<UNK>`. This proceeding has  a high impact in $PP$ given that different vocabularies (in terms of tokens and sizes) can achieve low values of $PP$ with more and less values of `<UNK>` words. That said, *one should only compare $PP$ with the same vocabulary*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Smoothing**\n",
    "\n",
    "What do we do with words that are in our vocabulary (they are not unknown words) but appear in a test set in an unseen context (for example they appear after a word they never appeared after in training)? Smoothing! This technique involves penalizing the probability of the higher end terms to provide some to those that are unlikely to happen in favor of having *some* for them.\n",
    "\n",
    "*Laplace smoothing*\n",
    "\n",
    "- Add 1 smothing:\n",
    "\n",
    "$P(w_i) = \\frac{c_i}{N} \\mapsto P^*(w_i) =  \\frac{c_i + 1}{N + V} = $\n",
    "\n",
    "Where:\n",
    "\n",
    "- $c^*_i = (c_i + 1) \\frac{N}{N+V}$.\n",
    "- $V$ is the number of words in the vocabulary.\n",
    "- $N$ is teh number of words in the corpus.\n",
    "\n",
    "\n",
    "- Discounting (lowering) as an alternative way of understanding how the probability mass concentrated in the high probable terms moves to the other less probable ones. There is a new magnitude called $d_c = \\frac{c^*}{c}$:\n",
    "\n",
    "$P(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n)}{C(w_{n-1})}$\n",
    "\n",
    "$P_{Laplace}(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n) + 1}{C(w_{n-1}) + V}$\n",
    "\n",
    "*Add-k smoothing*\n",
    "\n",
    "Exactly the same as Laplace smoothing, but instead of moving 1, we move k. That allows to better control the amount of discounted probability mass that is transfered to the zero terms.\n",
    "\n",
    "\n",
    "$P^*_{Add-k}(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n) + k}{C(w_{n-1}) + kV}$\n",
    "\n",
    "*Backoff and interpolation smoothing*\n",
    "\n",
    "The previous methods provide a certain ficticious count by extracting probability mass from highly concentrated terms to those that have none. Another way of doing this involves the special case of computing higher N-gram probabilities from those that have less order ( (N-m)-grams ) and then adjusting the probability distribution in favor of better distributing the probability of those higher end N-grams (interpolation):\n",
    "\n",
    "$P(w_n | w_{n-2}w_{n-1}) = \\lambda_1 P(w_n | w_{n-2}w_{n-1}) + \\lambda_2 P(w_n | w_{n-1}) + \\lambda_3 P(w_n)$\n",
    "\n",
    "$\\sum_{i=1}^3 \\lambda_i = 1$\n",
    "\n",
    "The vector of $\\lambda$ is learnt from a held-out set used to train these hyperparameters.\n",
    "\n",
    "This is useful when we have occurences of $w_{n-2} w_{n-1} w_n $, the entire N-gram. However, there are certain times where we have no occurrence at all of that, and we need to compute the probability by looking at what is near that, occurences of $w_{n-1} w_n $ or even $w_n $. This is nicely solved by *backing off* to the lower terms N-grams and *discounting* probability mass to spoil it in the higher terms that have nothing. See Katz backoff method for further details and also Good-Turing backoff algorithm.\n",
    "\n",
    "*KneserNey algorithm*\n",
    "\n",
    "The math can be seen in the book. The concept follows. It uses a really good intuition that is well expressed with an example. Suppose you want to predict which word follows in a sentence to complete. You can create a model from a unigram, bigram, trigram, etc. but all the terms have a great bias on the context and collocations. So, if we what to provide a better guess in an uknown context we want to penalize collocations in favor of a those unigrams used in a wider range of cases. This is what this method does. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
