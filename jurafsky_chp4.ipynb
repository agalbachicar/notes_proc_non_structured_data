{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech and Language Processing. Daniel Jurafsky & James H. Martin\n",
    "\n",
    "## Chapter 4 - Naive Bayes and Sentiment Classification\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "A Borges' cite.\n",
    "\n",
    "Text classification is the in core of many language processing tasks. One very common final application is sentiment analysis which is of remarkable importance for politics and marketing. The most basic form of sentiment analysis is the binart classification where we determine the humor of the writer as positive or negative with respect to certain topic. That classification can be even simpler, bu just dectecting certain *key* words text we can determine which is the feeling.\n",
    "\n",
    "Another applications range from spam detection, genre classification, domain areas classification, etc. etc.\n",
    "\n",
    "The chapter revolves around methods of supervised learning. Formally, the task of supervised classification is to take an input $x$ and a fixed set of output classes $Y = y1, y2,..., y_M$ and return a predicted class $y ∈ Y$. For text classification, we’ll sometimes talk about $c$ (for \"class\") instead of $y$ as our output variable, and $d$ (for \"document\") instead of $x$ as our input variable. In the supervised situation we have a training set of $N$ documents that have each been hand-labeled with a class: $(d1, c1),....,(d_N, c_N)$. Our goal is to learn a classifier that is capable of mapping from a new document $d$ to its correct class $c ∈ C$. A probabilistic classifier additionally will tell us the probability of the observation being in the class. This full distribution over the classes can be useful information for downstream decisions; avoiding making discrete decisions early on can be useful when combining systems.\n",
    "\n",
    "*Note on the classifiers*\n",
    "\n",
    "The text describes two types of classifiers: generative and discriminative. The chapter works with Naive Bayes as an example of a generative classifier. The procedure might be extended to other discriminative classifiers that have their own customized pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Classifier**\n",
    "\n",
    "Multinomial Naive Bayes Classifier. Given a corpus or collection of documents, we tear up all of them and put all ther tokens into a *bag-of-words* ( for what's coming, BoW) as an unordered list of words. Then, we convert that list into an unodered set of tuples $(w, c)$ where $w$ is the word and $c$ is the count.\n",
    "\n",
    "Naive Bayes is a probabilistic classifier, meaning that for a document $d$, out of all classes $c ∈ C$ the classifier returns the class $\\hat{c}$ which has the maximum posterior probability given the document. Notation `ˆ` means \"our estimate of the correct class\".\n",
    "\n",
    "$ \\hat{c} = \\underset{c \\in C}{\\operatorname{argmax}} P(c|d) $\n",
    "\n",
    "Using Bayes theorem, we know that: $P(c|d) = \\frac{P(d|c) P(c)}{P(d)}$. Given that $P(d)$ is always the same in the corpus, and to determine $\\hat{c}$ we care about the numerator only, we can rewrite the expression above as:\n",
    "\n",
    "$ \\hat{c} = \\underset{c \\in C}{\\operatorname{argmax}} P(c|d) = \\underset{c \\in C}{\\operatorname{argmax}} P(d|c) P(c)$\n",
    "\n",
    "The aforementioned expression can be thought of a maximization problem of the product between the likehood of a document given a certain class and the prior probability of that class. With that, there are two remaining assumptions that will make the model simpler to compute:\n",
    "\n",
    "1- Features of a document, i.e. tokens / words, do not contain any information in the collocation. This means that all the explanatory information of the document is perfectly encoded in words frequency so the BoW processing makes total sense for this purpose.\n",
    "2- The Bayes assumption of feature independence is vital to solve this problem: $P(f_1 f_2 ... f_n | c) = P(f_1 | c) P(f_2 | c)... P(f_n | c) = \\displaystyle \\prod_{i = 1}^{n} P(f_i | c)$\n",
    "\n",
    "Using that:\n",
    "\n",
    "$ \\hat{c}_{NB} =   \\underset{c \\in C}{\\operatorname{argmax}} P(c) \\displaystyle \\prod_{i = 1}^{n} P(f_i | c) $\n",
    "\n",
    "Which can be further customized for texts by replacing $n$ by word $postions$ and using $log$ probabilities given the natural sparse domain:\n",
    "\n",
    "$ \\hat{c}_{NB} =   \\underset{c \\in C}{\\operatorname{argmax}} logP(c) + \\displaystyle \\sum_{i \\in positions} log P(f_i | c) $\n",
    "\n",
    "*Training*\n",
    "\n",
    "- $\\hat{P}(c) = \\frac{N_c}{N_{doc}}$\n",
    "- $\\hat{P}(w_i|c) = \\frac{C(w_i, c)}{\\sum_{w \\in V} C(w, c)}$\n",
    "- We can also add some kind of smoothing to compute $\\hat{P}(w_i|c)$.\n",
    "- What do we do about words that occur in our test data but are not in our vocabulary at all because they did not occur in any training document in any class? The solution for such unknown words is to ignore them — remove them from the test document and not include any probability for them at all.\n",
    "- Some systems choose to completely ignore another class of words: stop words, very frequent words like *the* and *a*. This can be done by sorting the vocabulary by frequency in the training set, and defining the top 10–100 vocabulary entries as stop words, or alternatively by using one of the many pre-defined stop word list available online. Then every instance of these stop words are simply removed from both training and test documents as if they had never occurred. In most text classification applications, however, using a stop word list doesn’t improve performance, and so it is more common to make use of the entire vocabulary and not use a stop word list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tricks to deal with real life text**\n",
    "\n",
    "- A simple way to handle negation is to pre-append a `NOT_` to tokens after a negation token such as `{not, don't, didn't, never, etc.}` up to the next punctuation mark. That provides a really good baseline for models.\n",
    "- Multinomial Binary Bayes.\n",
    "- Use sentiment lexicons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
